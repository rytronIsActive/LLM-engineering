{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81f4331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()\n",
    "genai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful and friendly assistant that\\\n",
    "    helps the user with their queries.\\\n",
    "        You essentially serve the same purpose as Alexa or Siri.\\\n",
    "            Avoid going too in-depth for techinical or complicated questions such as coding, maths or science.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884dade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_openai(message, history):\n",
    "    messages =[{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    stream = openai.chat.completions.create(model = \"gpt-4o-mini\", messages = messages, stream = True)\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_claude(message, history):\n",
    "    \n",
    "    def _as_text(x):\n",
    "        if isinstance(x, str):\n",
    "            return x\n",
    "        if isinstance(x, list):\n",
    "            parts = []\n",
    "            for p in x:\n",
    "                if isinstance(p, str):\n",
    "                    parts.append(p)\n",
    "                elif isinstance(p, dict):\n",
    "                    # common shapes: {\"type\":\"text\",\"text\":\"...\"} or {\"text\":\"...\"}\n",
    "                    parts.append(p.get(\"text\", \"\"))\n",
    "            return \"\".join(parts)\n",
    "        return str(x) if x is not None else \"\"\n",
    "\n",
    "    sanitized = []\n",
    "\n",
    "    if history and isinstance(history[0], dict):\n",
    "        # type=\"messages\" style history\n",
    "        for m in history:\n",
    "            role = m.get(\"role\")\n",
    "            if role in (\"user\", \"assistant\"):\n",
    "                sanitized.append({\"role\": role, \"content\": _as_text(m.get(\"content\", \"\"))})\n",
    "    else:\n",
    "        # default ChatInterface history: list of (user, assistant) tuples\n",
    "        for u, a in (history or []):\n",
    "            if u:\n",
    "                sanitized.append({\"role\": \"user\", \"content\": _as_text(u)})\n",
    "            if a:\n",
    "                sanitized.append({\"role\": \"assistant\", \"content\": _as_text(a)})\n",
    "\n",
    "    # last turn from the UI\n",
    "    sanitized.append({\"role\": \"user\", \"content\": _as_text(message)})\n",
    "\n",
    "    # optional: drop empty messages (Claude dislikes empty content)\n",
    "    sanitized = [m for m in sanitized if m[\"content\"].strip()]\n",
    "\n",
    "    stream = claude.messages.create(\n",
    "        model= \"claude-3-5-haiku-latest\",\n",
    "        max_tokens=4000,\n",
    "        system=system_message,              # must be a plain string\n",
    "        messages=sanitized,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        # handle only text deltas\n",
    "        if getattr(chunk, \"type\", \"\") == \"content_block_delta\":\n",
    "            text = getattr(getattr(chunk, \"delta\", None), \"text\", None)\n",
    "            if text:\n",
    "                response += text\n",
    "                yield response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f243f039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chat_with_gemini(message, history):\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        'gemini-2.5-flash-lite',  \n",
    "        system_instruction=system_message \n",
    "    )\n",
    "    \n",
    "    gemini_history = []\n",
    "    for msg in history:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            gemini_history.append({\"role\": \"user\", \"parts\": [msg[\"content\"]]})\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            gemini_history.append({\"role\": \"model\", \"parts\": [msg[\"content\"]]})\n",
    "    \n",
    "    chat = model.start_chat(history=gemini_history)\n",
    "    response_stream = chat.send_message(message, stream=True)\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in response_stream:\n",
    "        if chunk.text:\n",
    "            response += chunk.text\n",
    "            yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8e7c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_llm_chat(message, history, model):\n",
    "    if model == \"OpenAI\":\n",
    "        yield from chat_with_openai(message, history)\n",
    "    elif model == \"Claude\":\n",
    "        yield from chat_with_claude(message, history)\n",
    "    elif model == \"Gemini\":\n",
    "        yield from chat_with_gemini(message, history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eed2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_interface():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Multi-LLM Assistant\")\n",
    "        gr.Markdown(\"Select your preferred AI model below\")\n",
    "        \n",
    "        model_dropdown = gr.Dropdown(\n",
    "            choices=[\"OpenAI\", \"Claude\", \"Gemini\"], \n",
    "            value=None,\n",
    "            label=\"Models\",\n",
    "            info=\"You must select a model\"\n",
    "        )\n",
    "        \n",
    "        gr.Markdown(\"Start your converstion once you've selected a model\")\n",
    "        \n",
    "        # Create the chat interface with the model as additional input\n",
    "        chat = gr.ChatInterface(\n",
    "            fn=multi_llm_chat,\n",
    "            additional_inputs=[model_dropdown],\n",
    "            type=\"messages\"\n",
    "        )\n",
    "    \n",
    "    return demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24eed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = create_custom_interface()\n",
    "view.launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view = gr.ChatInterface(\n",
    "#     fn=multi_llm_chat,\n",
    "#     additional_inputs=[\n",
    "#         gr.Dropdown(choices=[\"OpenAI\", \"Claude\", \"Gemini\"], value=\"None\", label=\"Select model\")\n",
    "#     ],\n",
    "#     type=\"messages\"\n",
    "# )\n",
    "\n",
    "# view.launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
